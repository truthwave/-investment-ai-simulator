# train_model.py

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE  # SMOTEå°å…¥
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.preprocessing import StandardScaler
import joblib
import warnings
warnings.filterwarnings('ignore')

# ç‰¹å¾´é‡ã‚’å®šç¾©ï¼ˆä½¿ç”¨ã™ã‚‹å…¨ã¦ã®ã‚«ãƒ©ãƒ åã‚’æ­£ç¢ºã«ï¼‰
features = [
    'open', 'high', 'low', 'close', 'volume', 'rsi', 'ma', 'ma_50', 'ma_200',
    'macd', 'macd_signal', 'macd_histogram', 'bb_upper', 'bb_middle', 'bb_lower',
    'bollinger_band_width', 'volatility', 'ma_diff', 'ma_trend', 'price_change',
    'price_range', 'volatility_ratio', 'ma_slope', 'rsi_slope', 'return_5',
    'return_10', 'rsi_change', 'close_mean_3', 'bb_width', 'rsi_volatility_combo',
    'macd_diff', 'ma_ratio', 'rsi_macd_diff', 'vol_bb_ratio', 'rsi_squared', 'macd_abs'
]

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = pd.read_csv("training_data.csv")
df = df.dropna()
X = df[features]
y = df["target"]

# ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–ï¼ˆSMOTEå‰ã«è¡Œã†ã¨è‰¯ã„çµæœã«ãªã‚Šã‚„ã™ã„ï¼‰
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# SMOTE é©ç”¨ï¼ˆã‚¯ãƒ©ã‚¹1åˆæˆï¼‰
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)
print(f"âœ… SMOTEé©ç”¨å¾Œã®ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {np.bincount(y_resampled)}")

# å­¦ç¿’ç”¨ãƒ»æ¤œè¨¼ç”¨ã«åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X_resampled, y_resampled, test_size=0.2, random_state=42
)

# ãƒ¢ãƒ‡ãƒ«å®šç¾©ï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« VotingClassifierï¼‰
xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=1.0, random_state=42)
lgb_clf = LGBMClassifier(random_state=42)
rf_clf = RandomForestClassifier(random_state=42)

ensemble_clf = VotingClassifier(
    estimators=[('xgb', xgb_clf), ('lgb', lgb_clf), ('rf', rf_clf)],
    voting='soft'
)

# ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
ensemble_clf.fit(X_train, y_train)

# äºˆæ¸¬ï¼ˆç¢ºç‡ï¼‰
y_pred_proba = ensemble_clf.predict_proba(X_test)[:, 1]

# æœ€é©ãªã—ãã„å€¤ã‚’æ¢ç´¢ï¼ˆ0.05ã€œ0.50ï¼‰
best_threshold = 0.5
best_f1 = 0
for th in np.arange(0.05, 0.50, 0.01):
    y_pred = (y_pred_proba > th).astype(int)
    report = classification_report(y_test, y_pred, output_dict=True)
    if report['1']['f1-score'] > best_f1:
        best_f1 = report['1']['f1-score']
        best_threshold = th

print(f"âœ… æœ€é©ã—ãã„å€¤: {best_threshold:.2f}")

# æœ€çµ‚è©•ä¾¡
y_pred_final = (y_pred_proba > best_threshold).astype(int)
print("âœ… ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’å®Œäº†ï¼ˆSMOTEé©ç”¨ï¼‰")
print(f"æ­£è§£ç‡: {accuracy_score(y_test, y_pred_final):.4f}")
print("æ··åŒè¡Œåˆ—:")
print(confusion_matrix(y_test, y_pred_final))
print("ğŸ“„ åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
print(classification_report(y_test, y_pred_final))

import joblib

# å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
joblib.dump(model, 'trained_model.pkl')
print("âœ… ãƒ¢ãƒ‡ãƒ«ã‚’ 'trained_model.pkl' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

