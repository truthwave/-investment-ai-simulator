# train_model.py

from sklearn.metrics import f1_score
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE  # SMOTEå°å…¥
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.preprocessing import StandardScaler
import joblib
import warnings
warnings.filterwarnings('ignore')


# ç‰¹å¾´é‡ã‚’å®šç¾©ï¼ˆä½¿ç”¨ã™ã‚‹å…¨ã¦ã®ã‚«ãƒ©ãƒ åã‚’æ­£ç¢ºã«ï¼‰
features = [
    'open', 'high', 'low', 'close', 'volume', 'rsi', 'ma', 'ma_50', 'ma_200',
    'macd', 'macd_signal', 'macd_histogram', 'bb_upper', 'bb_middle', 'bb_lower',
    'bollinger_band_width', 'volatility', 'ma_diff', 'ma_trend', 'price_change',
    'price_range', 'volatility_ratio', 'ma_slope', 'rsi_slope', 'return_5',
    'return_10', 'rsi_change', 'close_mean_3', 'bb_width', 'rsi_volatility_combo',
    'macd_diff', 'ma_ratio', 'rsi_macd_diff', 'vol_bb_ratio', 'rsi_squared', 'macd_abs'
]

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = pd.read_csv("training_data.csv")
df = df.dropna()
X = df[features]
y = df["target"]

# ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–ï¼ˆSMOTEå‰ã«è¡Œã†ã¨è‰¯ã„çµæœã«ãªã‚Šã‚„ã™ã„ï¼‰
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# SMOTE é©ç”¨ï¼ˆã‚¯ãƒ©ã‚¹1åˆæˆï¼‰
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)
print(f"âœ… SMOTEé©ç”¨å¾Œã®ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {np.bincount(y_resampled)}")

# å­¦ç¿’ç”¨ãƒ»æ¤œè¨¼ç”¨ã«åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X_resampled, y_resampled, test_size=0.2, random_state=42
)

# ãƒ¢ãƒ‡ãƒ«å®šç¾©ï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« VotingClassifierï¼‰
xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=1.0, random_state=42)
lgb_clf = LGBMClassifier(random_state=42)
rf_clf = RandomForestClassifier(random_state=42)
# ãƒ‘ãƒ¼ãƒˆï¼“ï¼šãƒ¢ãƒ‡ãƒ«ã®å®šç¾©ã¨å­¦ç¿’

# ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©
from sklearn.ensemble import VotingClassifier
import xgboost as xgb
import lightgbm as lgb
from backend.catboost import CatBoostClassifier

# å„ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
xgb_model = xgb.XGBClassifier(
    n_estimators=100, max_depth=5, learning_rate=0.1,
    scale_pos_weight=1.0, use_label_encoder=False, eval_metric='logloss'
)

lgbm_model = lgb.LGBMClassifier(
    n_estimators=100, max_depth=5, learning_rate=0.1,
    class_weight='balanced'
)

cat_model = CatBoostClassifier(
    iterations=100, depth=5, learning_rate=0.1,
    verbose=0, class_weights=[1.0, 3.0]
)

# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰
model = VotingClassifier(
    estimators=[
        ('xgb', xgb_model),
        ('lgbm', lgbm_model),
        ('cat', cat_model)
    ],
    voting='soft'
)

# å­¦ç¿’
model.fit(X_train, y_train)

ensemble_clf = VotingClassifier(
    estimators=[('xgb', xgb_clf), ('lgb', lgb_clf), ('rf', rf_clf)],
    voting='soft'
)

# ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
ensemble_clf.fit(X_train, y_train)


# äºˆæ¸¬ç¢ºç‡
y_pred_proba = model.predict(X_test)

# æœ€é©ãªã—ãã„å€¤ã‚’æ¢ç´¢
thresholds = np.arange(0.1, 0.9, 0.01)
f1_scores = []

for t in thresholds:
    preds = (y_pred_proba >= t).astype(int)
    f1 = f1_score(y_test, preds, average="macro")
    f1_scores.append(f1)

best_threshold = thresholds[np.argmax(f1_scores)]

# ãƒ‘ãƒ¼ãƒˆ: é«˜ãƒªã‚¹ã‚¯äºˆæ¸¬ã®é™¤å¤–ï¼ˆconfidence margin ã«åŸºã¥ããƒ•ã‚£ãƒ«ã‚¿ï¼‰
confidence_margin = 0.1  # ã“ã“ã‚’èª¿æ•´å¯èƒ½ï¼ˆä¾‹ï¼š0.05ï½0.3ãã‚‰ã„ï¼‰
threshold = best_threshold

y_pred_proba = model.predict_proba(X_test)[:, 1]
y_pred = (y_pred_proba >= threshold).astype(int)

# ä¸­å¤®0.5ã‹ã‚‰marginã‚’å¤–ã‚ŒãŸã€Œé«˜è‡ªä¿¡åº¦ã€ã ã‘ã‚’è©•ä¾¡å¯¾è±¡ã«ã™ã‚‹
confidence_mask = (y_pred_proba < (0.5 - confidence_margin)) | (y_pred_proba > (0.5 + confidence_margin))
y_pred_confident = y_pred[confidence_mask]
y_test_confident = y_test[confidence_mask]

# è‡ªä¿¡ã®ã‚ã‚‹äºˆæ¸¬ã ã‘æŠ½å‡º
X_eval = X_test[confidence_mask]
y_eval = y_test[confidence_mask]

# ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¾Œã®æ··åŒè¡Œåˆ—ã¨ãƒ¬ãƒãƒ¼ãƒˆ
print("âœ… é«˜ä¿¡é ¼äºˆæ¸¬ã®ã¿ã®è©•ä¾¡ï¼ˆconfidence margin =", confidence_margin, "ï¼‰")
print("è©•ä¾¡å¯¾è±¡ã‚µãƒ³ãƒ—ãƒ«æ•°:", len(y_test_confident))
print(confusion_matrix(y_test_confident, y_pred_confident))
print(classification_report(y_test_confident, y_pred_confident))

# æœ€é©ãªã—ãã„å€¤ï¼ˆä¾‹: 0.3ï¼‰ â€» GridSearchå¾Œã«è‡ªå‹•ã§æ±ºã¾ã‚‹ã‚ˆã†ã«ã—ã¦ã„ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ã†
optimal_threshold = 0.3
confidence_margin = 0.1  # è‡ªä¿¡åº¦ã®ã—ãã„å€¤å¹…ï¼ˆä¾‹: Â±10%ï¼‰

# æœ€é©ãª confidence_margin ã‚’æ¢ã™
best_margin = None
best_f1 = 0
best_report = ""
best_confusion = None

print("ğŸ“Š confidence_margin æœ€é©åŒ–é–‹å§‹...")
for margin in np.arange(0.05, 0.35, 0.01):  # 0.05ã€œ0.3 ã‚’ 0.01 åˆ»ã¿ã§è©•ä¾¡
    filtered_preds = []
    for p in y_pred_proba:
        if p >= optimal_threshold + margin:
            filtered_preds.append(1)
        elif p <= optimal_threshold - margin:
            filtered_preds.append(0)
        else:
            filtered_preds.append(-1)

    mask = np.array(filtered_preds) != -1
    y_eval = y_test[mask]
    y_pred_eval = np.array(filtered_preds)[mask]

    if len(y_eval) == 0:
        continue  # äºˆæ¸¬ãªã—

    f1 = f1_score(y_eval, y_pred_eval, average="macro")  # macro: ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã«å¯¾å¿œ
    if f1 > best_f1:
        best_f1 = f1
        best_margin = margin
        best_report = classification_report(y_eval, y_pred_eval)
        best_confusion = confusion_matrix(y_eval, y_pred_eval)

# çµæœå‡ºåŠ›
print(f"âœ… æœ€é© confidence_margin: {best_margin}")
print(f"âœ… æœ€å¤§ Macro F1-score: {round(best_f1, 4)}")
print("æ··åŒè¡Œåˆ—:")
print(best_confusion)
print("ğŸ“„ åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
print(best_report)

# è‡ªä¿¡ãŒã‚ã‚‹äºˆæ¸¬ã ã‘é€šã™ï¼ˆãã‚Œä»¥å¤–ã¯ -1 ã«ã™ã‚‹ï¼ä¸­ç«‹ï¼‰
y_pred_filtered = []
for proba in y_pred_proba:
    if proba >= optimal_threshold + confidence_margin:
        y_pred_filtered.append(1)  # å¼·ã„è²·ã„äºˆæ¸¬
    elif proba <= optimal_threshold - confidence_margin:
        y_pred_filtered.append(0)  # å¼·ã„å£²ã‚Šäºˆæ¸¬
    else:
        y_pred_filtered.append(-1)  # è‡ªä¿¡ãªã„ â†’ ä¸­ç«‹æ‰±ã„

# ä¸­ç«‹(-1) ã‚’é™¤å¤–ã—ãŸãƒ‡ãƒ¼ã‚¿ã ã‘ã§è©•ä¾¡
import numpy as np
import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# æœ‰åŠ¹ãªäºˆæ¸¬ã ã‘å–ã‚Šå‡ºã—
mask = np.array(y_pred_filtered) != -1
y_eval = y_test[mask]
y_pred_final = np.array(y_pred_filtered)[mask]

# æœ€é©ãªã—ãã„å€¤ã‚’æ¢ç´¢ï¼ˆ0.05ã€œ0.50ï¼‰
best_threshold = 0.5
best_f1 = 0
for th in np.arange(0.05, 0.50, 0.01):
    y_pred = (y_pred_proba > th).astype(int)
    report = classification_report(y_test, y_pred, output_dict=True)
    if report['1']['f1-score'] > best_f1:
        best_f1 = report['1']['f1-score']
        best_threshold = th

print(f"âœ… æœ€é©ã—ãã„å€¤: {best_threshold:.2f}")

# æœ€çµ‚è©•ä¾¡
y_pred_final = (y_pred_proba > best_threshold).astype(int)
print("âœ… ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’å®Œäº†ï¼ˆSMOTEé©ç”¨ï¼‰")
print(f"æ­£è§£ç‡: {accuracy_score(y_test, y_pred_final):.4f}")
print("æ··åŒè¡Œåˆ—:")
print(confusion_matrix(y_test, y_pred_final))
print("ğŸ“„ åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
print(classification_report(y_test, y_pred_final))

import joblib

# å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
joblib.dump(model, 'trained_model.pkl')
print("âœ… ãƒ¢ãƒ‡ãƒ«ã‚’ 'trained_model.pkl' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

# ãƒ‘ãƒ¼ãƒˆNï¼šå­¦ç¿’çµæœã®ãƒ­ã‚°ä¿å­˜
import os
import csv
from datetime import datetime
from sklearn.metrics import precision_score, recall_score, accuracy_score

# è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—ï¼ˆå¿…è¦ã«å¿œã˜ã¦å†è¨ˆç®—ï¼‰
acc_score = accuracy_score(y_test, y_pred)
precision_1 = precision_score(y_test, y_pred, pos_label=1)
recall_0 = recall_score(y_test, y_pred, pos_label=0)

if os.path.exists("training_log.csv"):
    os.remove("training_log.csv")
    print("ğŸ§¹ training_log.csv ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")

# ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
log_path = "training_log.csv"

from sklearn.metrics import accuracy_score

# ---- ãƒ•ã‚£ãƒ«ã‚¿å¯¾è±¡ã®ä¿¡é ¼åº¦ãƒã‚¹ã‚¯ã‚’ä½œæˆ ----
confidence_mask = (y_pred_proba >= 0.5 + best_margin) | (y_pred_proba <= 0.5 - best_margin)

# ---- ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®äºˆæ¸¬ãƒ»æ­£è§£ãƒ©ãƒ™ãƒ«ã‚’æŠ½å‡º ----
y_pred_filtered = y_pred[confidence_mask]
y_test_filtered = y_test[confidence_mask]

# äºˆæ¸¬ã¨æ­£è§£ãƒ©ãƒ™ãƒ«ã‹ã‚‰ accuracy ã‚’è¨ˆç®—
accuracy = accuracy_score(y_test_filtered, y_pred_filtered)

macro_f1 = report["macro avg"]["f1-score"]

log_df = pd.DataFrame([{
    "datetime": pd.Timestamp.now(),
    "accuracy": accuracy,
    "macro_f1": macro_f1,
    "confidence_margin": best_margin
}])

log_df.to_csv(
    log_path,
    mode='a',
    index=False,
    header=not os.path.exists(log_path) 
)
# ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿è¡Œ
log_fields = ["datetime", "accuracy", "precision_1", "recall_0"]
now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
new_row = [now, acc_score, precision_1, recall_0]

# æ›¸ãè¾¼ã¿å‡¦ç†ï¼ˆè¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ï¼‰
file_exists = os.path.isfile(log_path)
with open(log_path, mode="a", newline="", encoding="utf-8") as f:
    writer = csv.writer(f)
    if not file_exists:
        writer.writerow(log_fields)
    writer.writerow(new_row)

print(f"ğŸ“ ãƒ­ã‚°ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {log_path}")

# ãƒ‘ãƒ¼ãƒˆ6ï¼šç²¾åº¦æ¯”è¼ƒã¨ä¿å­˜å‡¦ç†ï¼ˆãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆåˆ¤å®šï¼‰
import os
import joblib

if os.path.exists("training_log.csv"):
    os.remove("training_log.csv")
    print("ğŸ§¹ training_log.csv ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")

log_path = "training_log.csv"
old_model_path = "trained_model.pkl"
new_model_path = "trained_model_new.pkl"

# æœ€æ–°ã®æ—§ãƒ¢ãƒ‡ãƒ«ã‚¹ã‚³ã‚¢ã‚’å–å¾—
def get_last_f1_from_log(log_path):
    if not os.path.exists(log_path):
        return 0.0
    import pandas as pd
    df_log = pd.read_csv(log_path)
    if 'macro_f1' not in df_log.columns:
        return 0.0
    return df_log['macro_f1'].iloc[-1]

# confidence_margin æœ€é©åŒ–å¾Œ
threshold = best_threshold  # æœ€é©ã—ãã„å€¤ã‚’ä½¿ã†

# é«˜ä¿¡é ¼åº¦ãƒ‡ãƒ¼ã‚¿ã ã‘ã«çµã‚‹ï¼ˆä¾‹ï¼š0.5 Â± confidence_margin ä»¥å¤–ï¼‰
confidence_mask = (y_pred_proba < (0.5 - confidence_margin)) | (y_pred_proba > (0.5 + confidence_margin))
X_eval = X_test[confidence_mask]
y_eval = y_test[confidence_mask]

# çµã£ãŸãƒ‡ãƒ¼ã‚¿ã§å†åº¦äºˆæ¸¬
y_pred_eval = (y_pred_proba[confidence_mask] >= best_threshold).astype(int)

# === confidence_margin æœ€é©åŒ– ===

# ä¿¡é ¼åº¦ãƒãƒ¼ã‚¸ãƒ³ã‚’ç”¨ã„ãŸäºˆæ¸¬ã®ä¿¡é ¼é ˜åŸŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è©•ä¾¡
from sklearn.metrics import classification_report

print("ğŸ“Š confidence_margin æœ€é©åŒ–é–‹å§‹...")

best_margin = 0.0
best_macro_f1 = 0.0
best_report = None
best_cm = None

for margin in np.arange(0.05, 0.5, 0.01):
    # æœ€çµ‚çš„ãªæœ€é©ãƒãƒ¼ã‚¸ãƒ³ã§ã®äºˆæ¸¬çµæœã‚’å–å¾—
    confidence_mask = (y_pred_proba < (0.5 - best_margin)) | (y_pred_proba > (0.5 + best_margin))
    best_y_pred_confident = y_pred[confidence_mask]
    best_y_test_confident = y_test.values[confidence_mask]

    # è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã‚’å†å‡ºåŠ›
    print("æ··åŒè¡Œåˆ—:")
    print(confusion_matrix(best_y_test_confident, best_y_pred_confident))
    print("ğŸ“„ åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
    print(classification_report(best_y_test_confident, best_y_pred_confident))

    # ãƒ­ã‚°ä¿å­˜ï¼ˆå®‰å…¨ã«ã‚­ãƒ¼ç¢ºèªï¼‰
    report = classification_report(best_y_test_confident, best_y_pred_confident, output_dict=True)
    precision_0 = report.get("0", {}).get("precision", 0.0)
    recall_0 = report.get("0", {}).get("recall", 0.0)
    f1_0 = report.get("0", {}).get("f1-score", 0.0)
    precision_1 = report.get("1", {}).get("precision", 0.0)
    recall_1 = report.get("1", {}).get("recall", 0.0)
    f1_1 = report.get("1", {}).get("f1-score", 0.0)


# å®‰å…¨ã«å„å€¤ã‚’å–å¾—ï¼ˆã‚­ãƒ¼ãŒç„¡ã„å ´åˆã¯0.0ï¼‰
report = best_report or {}
precision_0 = report.get("0", {}).get("precision", 0.0)
recall_0 = report.get("0", {}).get("recall", 0.0)
f1_0 = report.get("0", {}).get("f1-score", 0.0)
precision_1 = report.get("1", {}).get("precision", 0.0)
recall_1 = report.get("1", {}).get("recall", 0.0)
f1_1 = report.get("1", {}).get("f1-score", 0.0)

# è‡ªä¿¡åº¦ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã•ã‚ŒãŸäºˆæ¸¬ã¨å®Ÿéš›ã®æ­£è§£ãƒ©ãƒ™ãƒ«
confidence_mask = (y_pred_proba < (0.5 - best_margin)) | (y_pred_proba > (0.5 + best_margin))
y_pred_filtered = y_pred[confidence_mask]
y_true_filtered = y_test.values[confidence_mask]

# çµæœè¡¨ç¤º
print("æ··åŒè¡Œåˆ—:")
print(confusion_matrix(y_true_filtered, y_pred_filtered))
print("ğŸ“„ åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
print(classification_report(y_true_filtered, y_pred_filtered))


print(classification_report(y_true_filtered, y_pred_filtered))

# === confidence_margin ã®è©•ä¾¡ãƒ­ã‚°ã‚’ä¿å­˜ ===
log_file = "training_log.csv"
log_fields = [
    "datetime", "accuracy", "precision_0", "recall_0", "f1_0",
    "precision_1", "recall_1", "f1_1",
    "macro_f1", "confidence_margin", "n_confident"
]

now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
report = classification_report(best_y_test_confident, best_y_pred_confident, output_dict=True)

log_data = {
    "datetime": now,
    "accuracy": accuracy_score(best_y_test_confident, best_y_pred_confident),
    "precision_0": report["0"]["precision"],
    "recall_0": report["0"]["recall"],
    "f1_0": report["0"]["f1-score"],
    "precision_1": report["1"]["precision"],
    "recall_1": report["1"]["recall"],
    "f1_1": report["1"]["f1-score"],
    "macro_f1": best_f1,
    "confidence_margin": best_margin,
    "n_confident": len(best_y_test_confident),
}

# === CSVãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ãŒãªã‘ã‚Œã°è¿½åŠ ï¼‰ ===
file_exists = os.path.exists(log_file)
with open(log_file, mode="a", newline="", encoding="utf-8") as f:
    writer = csv.DictWriter(f, fieldnames=log_fields)
    if not file_exists:
        writer.writeheader()
    writer.writerow(log_data)

print(f"ğŸ“ confidence_margin ã®çµæœãƒ­ã‚°ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {log_file}")

# æ–°æ—§æ¯”è¼ƒã—ã¦ä¿å­˜åˆ¤æ–­
macro_f1 = best_f1  # â† ä¿®æ­£ï¼šã“ã®è¡Œã‚’ä¸Šã«ç§»å‹•ã—ã¦å…ˆã«å®šç¾©
last_macro_f1 = get_last_f1_from_log(log_path)
print(f"æ—§ãƒ¢ãƒ‡ãƒ« Macro F1: {last_macro_f1:.4f} / æ–°ãƒ¢ãƒ‡ãƒ« Macro F1: {macro_f1:.4f}")

if macro_f1 > last_macro_f1:
    joblib.dump(model, old_model_path)
    print("âœ… æ–°ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸï¼ˆæ—§ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šè‰¯å¥½ï¼‰")
else:
    print("âš ï¸ æ—§ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šç²¾åº¦ãŒä½ã„ãŸã‚ä¿å­˜ã—ã¾ã›ã‚“")
